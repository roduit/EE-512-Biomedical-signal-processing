{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3d11189-b4fa-402d-92c1-e23d5e258ba6",
   "metadata": {},
   "source": [
    "# Atrial Fibrillation Classification\n",
    "\n",
    "The goal of this exercise is to train different neural networks to discriminate between atrial fibrillation and normal sinus rhythm from a sequence of interbeat intervals. We use interbeat intervals extracted from the Long Term AF Database (https://physionet.org/content/ltafdb/1.0.0/).\n",
    "\n",
    "We will train the following models on windows of interbeat intervals:\n",
    "\n",
    "* Logistic regression\n",
    "* Multi-layer perceptron\n",
    "* Convolutional neural network\n",
    "* Recurrent neural network\n",
    "\n",
    "In addition, the first two models will also be trained on simple features derived from each window of interbeat intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43271ad3-6e0c-410b-b142-acce27564081",
   "metadata": {},
   "source": [
    "First, we import all the required packages, define global constants, and seed the random number generators to obtain reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b06315-c85f-4fe9-8a73-702e99267d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "\n",
    "import itertools\n",
    "import logging\n",
    "import operator\n",
    "import pathlib\n",
    "import warnings\n",
    "\n",
    "import IPython.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import scipy.special\n",
    "import seaborn as sns\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "import torch\n",
    "\n",
    "\n",
    "DATA_FILE = pathlib.Path('../data/ltafdb_intervals.npz')\n",
    "LOG_DIRECTORY = pathlib.Path('../logs/af_classification')\n",
    "\n",
    "\n",
    "# Disable logging for PyTorch Lightning to avoid too long outputs.\n",
    "logging.getLogger('pytorch_lightning').setLevel(logging.ERROR)\n",
    "\n",
    "# Seed random number generators for reproducible results.\n",
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b7435e-81d1-4c73-b7b0-705e4ea080e1",
   "metadata": {},
   "source": [
    "Then, we load the windows of interbeat intervals and the corresponding labels. We also load the record identifiers. They will help to avoid using intervals from the same record for both training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18674b81-5bde-4f16-b642-98f960de1f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    with np.load(DATA_FILE) as data:\n",
    "        intervals = data['intervals']\n",
    "        labels = data['labels']\n",
    "        identifiers = data['identifiers']\n",
    "    return intervals, labels, identifiers\n",
    "\n",
    "\n",
    "intervals, labels, identifiers = load_data()\n",
    "targets = (labels == 'atrial_fibrillation').astype('float32')[:, None]\n",
    "window_size = intervals.shape[1]\n",
    "\n",
    "print(f'Number of windows: {intervals.shape[0]}')\n",
    "print(f'Window size: {window_size}')\n",
    "print(f'Window labels: {set(labels)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e0db28-03c0-4ef9-86db-27958f935fca",
   "metadata": {},
   "source": [
    "Here are a few examples of windows of interbeat intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eb179c-5887-4562-b26a-b2fb049e1eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_interval_examples(intervals, targets, n_examples=3):\n",
    "    normal_indices = np.random.choice(np.flatnonzero(targets == 0.0), n_examples, replace=False)\n",
    "    af_indices = np.random.choice(np.flatnonzero(targets == 1.0), n_examples, replace=False)\n",
    "    \n",
    "    def plot_intervals(ax, index):\n",
    "        ax.plot(np.cumsum(intervals[index]), intervals[index], '.-')\n",
    "        ax.grid(True)\n",
    "    \n",
    "    fig, axes = plt.subplots(n_examples, 2, sharex='all', sharey='all', squeeze=False, constrained_layout=True)\n",
    "    for i in range(n_examples):\n",
    "        plot_intervals(axes[i, 0], normal_indices[i])\n",
    "        plot_intervals(axes[i, 1], af_indices[i])\n",
    "    plt.setp(axes, ylim=(0.0, 3.0))\n",
    "    plt.setp(axes[-1, :], xlabel='Time [s]')\n",
    "    plt.setp(axes[:, 0], ylabel='IBI [s]')\n",
    "    axes[0, 0].set_title('Normal rhythm')\n",
    "    axes[0, 1].set_title('Atrial fibrillation')\n",
    "    \n",
    "    \n",
    "plot_interval_examples(intervals, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4947482-6ffd-4e36-9f67-b5aa16a61218",
   "metadata": {},
   "source": [
    "**Question 1**\n",
    "\n",
    "Visually, what are the differences between the examples of the two classes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe3d08a-68e9-4a3f-98f4-31795e87cd89",
   "metadata": {},
   "source": [
    "The next step is to split the dataset into subsets for training, validation, and testing stratified by labels. We use the record identifiers to avoid using windows from the same record in more than one subest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5126c623-7fb3-4b21-a6d9-e53c96fa3e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(identifiers, intervals, targets):\n",
    "    splitter = sklearn.model_selection.StratifiedGroupKFold(n_splits=5)\n",
    "    indices = list(map(operator.itemgetter(1), splitter.split(intervals, targets, identifiers)))\n",
    "    i_train = np.hstack(indices[:-2])\n",
    "    i_val = indices[-2]\n",
    "    i_test = indices[-1]\n",
    "    \n",
    "    assert not (set(identifiers[i_train]) & set(identifiers[i_val]))\n",
    "    assert not (set(identifiers[i_train]) & set(identifiers[i_test]))\n",
    "    assert not (set(identifiers[i_val]) & set(identifiers[i_test]))\n",
    "    assert set(identifiers[i_train]) | set(identifiers[i_val]) | set(identifiers[i_test]) == set(identifiers)\n",
    "    \n",
    "    return i_train, i_val, i_test\n",
    "\n",
    "\n",
    "i_train, i_val, i_test = split_data(identifiers, intervals, targets)\n",
    "\n",
    "\n",
    "def build_summary(subsets, targets):\n",
    "    data = []\n",
    "    for subset, y in zip(subsets, targets):\n",
    "        data.append({\n",
    "            'subset': subset,\n",
    "            'total_count': y.size,\n",
    "            'normal_count': np.sum(y == 0.0),\n",
    "            'af_count': np.sum(y == 1.0),\n",
    "            'normal_proportion': np.mean(y == 0.0),\n",
    "            'af_proportion': np.mean(y == 1.0),\n",
    "        })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "IPython.display.display(build_summary(('training', 'validation', 'testing'), (targets[i_train], targets[i_val], targets[i_val])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a1effb-438a-4d15-9d77-74693947a4e6",
   "metadata": {},
   "source": [
    "To better understand the dataset, we extract two features from each window of interbeat intervals: the mean and the standard deviation. We then plot these two features for the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ee64b7-bbd0-4e09-9b38-dcd838a84fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.vstack((\n",
    "    np.mean(intervals, axis=1), \n",
    "    np.std(intervals, axis=1),\n",
    ")).T\n",
    "\n",
    "\n",
    "def plot_features(f, y):\n",
    "    data = pd.DataFrame({\n",
    "        'label': y.ravel(),\n",
    "        'mean': f[:, 0],\n",
    "        'std': f[:, 1],\n",
    "    })\n",
    "    data['label'] = data['label'].map({0.0: 'normal_rhythm', 1.0: 'atrial_fibrillation'})\n",
    "    sns.pairplot(data, hue='label', plot_kws={'s': 4})\n",
    "    \n",
    "    \n",
    "plot_features(features[i_train], targets[i_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d80c68-e0b1-4063-a796-9123d2aec79d",
   "metadata": {},
   "source": [
    "**Question 2**\n",
    "\n",
    "Would it be possible to discriminate between the two classes with these features and a linear classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8c92eb-0398-40eb-86ec-1482ef5bed10",
   "metadata": {},
   "source": [
    "To classify atrial fibrillation and normal rhythm, we define three models: a multi-layer perceptron (MLP), a convolutional neural network (CNN), and a recurrent neural network (RNN).\n",
    "\n",
    "\n",
    "Make sure you understand the differences between these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041a16d1-cfc3-4fb4-9909-845cc36171e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlpModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, n_hidden_layers=1, n_units=128):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.n_units = n_units\n",
    "        self.layers = self._build_layers()\n",
    "        \n",
    "    def _build_layers(self):\n",
    "        sizes = [self.input_size]\n",
    "        sizes.extend(itertools.repeat(self.n_units, self.n_hidden_layers))\n",
    "        layers = []\n",
    "        for i in range(self.n_hidden_layers):\n",
    "            layers.append(torch.nn.Linear(sizes[i], sizes[i + 1]))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "        layers.append(torch.nn.Linear(sizes[-1], self.output_size))\n",
    "        return torch.nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "    \n",
    "class CnnModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 input_size,\n",
    "                 output_size,\n",
    "                 n_layers=3,\n",
    "                 n_initial_channels=8,\n",
    "                 kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.n_initial_channels = n_initial_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.layers = self._build_layers()\n",
    "\n",
    "    def _build_layers(self):\n",
    "        layers = []\n",
    "\n",
    "        # Build convolutional layers. Each layer is composed of a convolution,\n",
    "        # a ReLU activation, and max pooling (except for the last convolutional\n",
    "        # layer that uses global average pooling).\n",
    "        n_output_channels = 1\n",
    "        for i in range(self.n_layers):\n",
    "            n_input_channels = n_output_channels\n",
    "            n_output_channels = self.n_initial_channels * 2 ** i\n",
    "            layers.append(torch.nn.Conv1d(\n",
    "                in_channels=n_input_channels,\n",
    "                out_channels=n_output_channels,\n",
    "                kernel_size=self.kernel_size,\n",
    "                padding='same',\n",
    "            ))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            if i < self.n_layers - 1:\n",
    "                layers.append(torch.nn.MaxPool1d(kernel_size=2))\n",
    "            else:\n",
    "                layers.append(torch.nn.AdaptiveAvgPool1d(1))\n",
    "        layers.append(torch.nn.Flatten())\n",
    "\n",
    "        # Build output layer.\n",
    "        layers.append(torch.nn.Linear(\n",
    "            in_features=self.n_initial_channels * 2 ** (self.n_layers - 1),\n",
    "            out_features=self.output_size,\n",
    "        ))\n",
    "\n",
    "        return torch.nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], 1, x.shape[1])\n",
    "        return self.layers(x)\n",
    "    \n",
    "\n",
    "class RnnModel(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size, hidden_size=64):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.recurrent_layer = torch.nn.GRU(\n",
    "            input_size=1,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=1,\n",
    "            batch_first=False,\n",
    "        )\n",
    "        self.output_layer = torch.nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(0, 1)[..., None]\n",
    "        y, h = self.recurrent_layer(x)\n",
    "        return self.output_layer(y[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bc8d2a-9ada-4468-aea3-36aedb560470",
   "metadata": {},
   "source": [
    "We also define a class to manage different models and a few functions to train and evaluate models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3662c9d-0441-4543-ad08-2cdce59e106b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.config = config\n",
    "        self.model = self._build_model()\n",
    "        self.example_input_array = torch.zeros((1, self.model.input_size))\n",
    "        \n",
    "    def _build_model(self):\n",
    "        name = self.config['model']['name']\n",
    "        config = self.config['model'].get('config', {})\n",
    "        if name == 'mlp':\n",
    "            return MlpModel(**config)\n",
    "        elif name == 'cnn':\n",
    "            return CnnModel(**config)\n",
    "        elif name == 'rnn':\n",
    "            return RnnModel(**config)\n",
    "        else:\n",
    "            raise ValueError(f'unknown model: {name!r}')\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        name = self.config['optimizer']['name']\n",
    "        config = self.config['optimizer'].get('config', {})\n",
    "        if name == 'sgd':\n",
    "            return torch.optim.SGD(self.parameters(), **config)\n",
    "        elif name == 'adam':\n",
    "            return torch.optim.Adam(self.parameters(), **config)\n",
    "        else:\n",
    "            raise ValueError(f'unknown optimizer: {name!r}')\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._run_step(batch, 'train')\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._run_step(batch, 'val')\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self._run_step(batch, 'test')\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        x, y = batch\n",
    "        return self.model(x)\n",
    "\n",
    "    def _run_step(self, batch, subset):\n",
    "        x, y = batch\n",
    "        logits = self.model(x)\n",
    "        loss = torch.nn.functional.binary_cross_entropy_with_logits(logits, y)\n",
    "        acc = ((logits > 0.0).float() == y).float().mean()\n",
    "        self.log_dict({f'{subset}_loss': loss, f'{subset}_acc': acc},\n",
    "                      on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    \n",
    "def build_loader(*tensors, batch_size=100, shuffle=False, n_workers=0):\n",
    "    dataset = torch.utils.data.TensorDataset(*map(torch.Tensor, tensors))\n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=n_workers,\n",
    "    )\n",
    "\n",
    "def train_model(name, config, x, y, i_train, i_val, n_epochs=10, batch_size=100):\n",
    "    train_loader = build_loader(x[i_train], y[i_train], batch_size=batch_size, shuffle=True)\n",
    "    val_loader = build_loader(x[i_val], y[i_val], batch_size=batch_size)\n",
    "    classifier = Classifier(config)\n",
    "    print(pl.utilities.model_summary.ModelSummary(classifier, max_depth=-1))\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        trainer = pl.Trainer(\n",
    "            default_root_dir=LOG_DIRECTORY,\n",
    "            logger=pl.loggers.TensorBoardLogger(LOG_DIRECTORY, name),\n",
    "            enable_model_summary=False,\n",
    "            max_epochs=n_epochs,\n",
    "        )\n",
    "        trainer.fit(classifier, train_loader, val_loader)\n",
    "\n",
    "    return classifier\n",
    "\n",
    "\n",
    "def compute_metrics(targets, predictions, threshold=0.5):\n",
    "    targets = np.ravel(targets) > threshold\n",
    "    predictions = np.ravel(predictions) > threshold\n",
    "    c = sklearn.metrics.confusion_matrix(targets, predictions)\n",
    "    tp = c[1, 1]\n",
    "    tn = c[0, 0]\n",
    "    fp = c[0, 1]\n",
    "    fn = c[1, 0]\n",
    "    return {\n",
    "        'count': c.sum(),\n",
    "        'acc': (tp + tn) / (tp + tn + fp + fn),\n",
    "        'tpr': tp / (tp + fn),\n",
    "        'tnr': tn / (tn + fp),\n",
    "        'ppv': tp / (tp + fp),\n",
    "        'npv': tn / (tn + fn),\n",
    "        'f1': 2 * tp / (2 * tp + fp + fn),\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_model(model, x, y, i_train, i_val, i_test, batch_size=100):\n",
    "    loader = build_loader(x, y, batch_size=batch_size)\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        trainer = pl.Trainer(\n",
    "            default_root_dir=LOG_DIRECTORY, \n",
    "            logger=False,\n",
    "            enable_progress_bar=False,\n",
    "            enable_model_summary=False,\n",
    "        )\n",
    "        z = trainer.predict(model, loader)\n",
    "    z = np.vstack([u.numpy() for u in z])\n",
    "    z = scipy.special.expit(z)  # Convert logits to probabilities.\n",
    "        \n",
    "    metrics = []\n",
    "    for subset, indices in (('train', i_train), ('val', i_val), ('test', i_test)):\n",
    "        metrics.append({\n",
    "            'subset': subset,\n",
    "            **compute_metrics(y[indices], z[indices]),\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1599366-82c7-4b18-b88c-3454d416e7b6",
   "metadata": {},
   "source": [
    "The final step before training and evaluating models is to define the configurations of the different models.\n",
    "\n",
    "We will train the following models:\n",
    "\n",
    "* Features as inputs (input size = 2):\n",
    "  * Logsitic regression\n",
    "  * Multi-layer perceptron\n",
    "    * Dense layer (output size = 128)\n",
    "    * ReLU activation\n",
    "    * Dense layer (output size = 128)\n",
    "    * ReLU activation\n",
    "    * Dense layer (output size = 1)\n",
    "* Interbeat intervals as inputs (input_size = 32):\n",
    "    * Logsitic regression\n",
    "    * Multi-layer perceptron\n",
    "      * Dense layer (output size = 128)\n",
    "      * ReLU activation\n",
    "      * Dense layer (output size = 128)\n",
    "      * ReLU activation\n",
    "      * Dense layer (output size = 1)\n",
    "    * Convolutional neural network\n",
    "      * Convolutional layer (kernel size = 3, output size = 32, output channels = 8)\n",
    "      * ReLU activation\n",
    "      * Max pooling (output size = 16, output channels = 8)\n",
    "      * Convolutional layer (kernel size = 3, output size = 16, output channels = 16)\n",
    "      * ReLU activation\n",
    "      * Max pooling (output size = 8, output channels = 16)\n",
    "      * Convolutional layer (kernel size = 3, output size = 8, output channels = 32)\n",
    "      * ReLU activation\n",
    "      * Global average pooling (output size = 32)\n",
    "      * Dense layer (output size = 1)\n",
    "    * Recurrent neural network\n",
    "      * GRU layer (output size = 64)\n",
    "      * Dense layer (output size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902ea0c6-bf6e-49d2-808a-35cac359ddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 40\n",
    "batch_size = 200\n",
    "\n",
    "configs = {\n",
    "    'features_logistic': {\n",
    "        'model': {\n",
    "            'name': 'mlp',\n",
    "            'config': {\n",
    "                'input_size': features.shape[1], \n",
    "                'output_size': targets.shape[1],\n",
    "                'n_hidden_layers': 0,  # A muli-layer perceptron without hidden layers is just a logistic regression.\n",
    "            },\n",
    "        },\n",
    "        'optimizer': {\n",
    "            'name': 'adam',\n",
    "            'config': {'lr': 0.01},\n",
    "        },\n",
    "    },\n",
    "    'features_mlp': {\n",
    "        'model': {\n",
    "            'name': 'mlp',\n",
    "            'config': {\n",
    "                'input_size': features.shape[1], \n",
    "                'output_size': targets.shape[1],\n",
    "                'n_hidden_layers': 2,\n",
    "                'n_units': 128,\n",
    "            },\n",
    "        },\n",
    "        'optimizer': {\n",
    "            'name': 'adam',\n",
    "            'config': {'lr': 0.001},\n",
    "        },\n",
    "    },\n",
    "    'logistic': {\n",
    "        'model': {\n",
    "            'name': 'mlp',\n",
    "            'config': {\n",
    "                'input_size': intervals.shape[1], \n",
    "                'output_size': targets.shape[1],\n",
    "                'n_hidden_layers': 0,  # A muli-layer perceptron without hidden layers is just a logistic regression.\n",
    "            },\n",
    "        },\n",
    "        'optimizer': {\n",
    "            'name': 'adam',\n",
    "            'config': {'lr': 0.001},\n",
    "        },\n",
    "    },\n",
    "    'mlp': {\n",
    "        'model': {\n",
    "            'name': 'mlp',\n",
    "            'config': {\n",
    "                'input_size': intervals.shape[1], \n",
    "                'output_size': targets.shape[1],\n",
    "                'n_hidden_layers': 2,\n",
    "                'n_units': 128,\n",
    "            },\n",
    "        },\n",
    "        'optimizer': {\n",
    "            'name': 'adam',\n",
    "            'config': {'lr': 0.001},\n",
    "        },\n",
    "    },\n",
    "    'cnn': {\n",
    "        'model': {\n",
    "            'name': 'cnn',\n",
    "            'config': {\n",
    "                'input_size': intervals.shape[1], \n",
    "                'output_size': targets.shape[1],\n",
    "                'n_layers': 3, \n",
    "                'n_initial_channels': 8,\n",
    "                'kernel_size': 3,\n",
    "            },\n",
    "        },\n",
    "        'optimizer': {\n",
    "            'name': 'adam',\n",
    "            'config': {'lr': 0.001},\n",
    "        },\n",
    "    },\n",
    "    'rnn': {\n",
    "        'model': {\n",
    "            'name': 'rnn',\n",
    "            'config': {\n",
    "                'input_size': intervals.shape[1], \n",
    "                'output_size': targets.shape[1],\n",
    "                'hidden_size': 64,\n",
    "            },\n",
    "        },\n",
    "        'optimizer': {\n",
    "            'name': 'adam',\n",
    "            'config': {'lr': 0.001},\n",
    "        },\n",
    "    },\n",
    "                \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e25ad5-d69b-4426-b94f-941f54b41f3d",
   "metadata": {},
   "source": [
    "To visualize the loss and accuracy during training, we start TensorBoard.\n",
    "\n",
    "If you prefer to view TensorBoard in a separate window, you can open http://localhost:6006/ in your web browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7e7dc2-8353-4ffe-9a26-b697455662f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir ../logs/af_classification --port 6006"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b43c95-98ac-46c3-8590-ffd666ed1bb1",
   "metadata": {},
   "source": [
    "Finally, we train the different models. It will take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d6fe59-6b0f-421c-a8ba-7dc723db179d",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "for name, config in configs.items():\n",
    "    print(f'Training {name!r} model')\n",
    "    x = features if name.startswith('features') else intervals\n",
    "    y = targets\n",
    "    models[name] = train_model(name, config, x, y, i_train, i_val, n_epochs, batch_size)\n",
    "    print('\\n\\n\\n')  # Add blank lines to separate models in the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb63c4df-c65c-472c-9466-fbfff0d9673b",
   "metadata": {},
   "source": [
    "**Question 3**\n",
    "\n",
    "Based on the model summaries printed above and the metrics shown in TensorBoard, answer the following questions:\n",
    "\n",
    "1. After training is finished, you can see the number of parameters for each model. Why is the number of parameters for the CNN model much lower than for the MLP model?\n",
    "2. What can you about the loss and accuracy of the different models on the training and validation subsets? Do some model overfit?\n",
    "3. Why does the logistic regression that takes features as inputs performs much better than the logistic regression that takes raw interbeat intervals as inputs?\n",
    "4. Are there models that would benefit from training for more epochs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9df4a6-6087-4a0c-96f5-a8455ca119b7",
   "metadata": {},
   "source": [
    "Now that all models are trained we can evaluate them on the subsets for training, validation, and testing. For each model, we compute the following metrics:\n",
    "\n",
    "* Accuracy (ACC)\n",
    "* True positive rate (TPR)\n",
    "* True negative rate (TNR)\n",
    "* Positive predictive value (PPV)\n",
    "* Negative predictive value (NPV)\n",
    "* F1 score (F1)\n",
    "\n",
    "You can find more information about these metrics on [Wikipedia](https://en.wikipedia.org/wiki/Confusion_matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0ea3e8-90c7-409e-88c6-9ba669665e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "for name, model in models.items():\n",
    "    x = features if name.startswith('features') else intervals\n",
    "    y = targets\n",
    "    df = evaluate_model(model, x, y, i_train, i_val, i_test, batch_size=batch_size)\n",
    "    df.insert(0, 'model', name)\n",
    "    metrics.append(df)\n",
    "metrics = pd.concat(metrics, axis=0, ignore_index=True)\n",
    "metrics = metrics.set_index(['model', 'subset'])\n",
    "index = metrics.index.get_level_values(0).unique()\n",
    "columns = pd.MultiIndex.from_product([metrics.columns, metrics.index.get_level_values(1).unique()])\n",
    "metrics = metrics.unstack().reindex(index=index, columns=columns)\n",
    "IPython.display.display(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5e5624-c3bc-474d-9fec-dbc51751d7ae",
   "metadata": {},
   "source": [
    "We can also plot the different metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14940ec-9126-4830-9a55-38a100665abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(data):\n",
    "    for metric in data.columns.get_level_values(0).unique():\n",
    "        if metric == 'count':\n",
    "            continue\n",
    "        df = data[metric]\n",
    "        plt.figure(constrained_layout=True)\n",
    "        plt.gca().set_axisbelow(True)\n",
    "        df.plot(kind='bar', ylabel=metric, ax=plt.gca())\n",
    "        plt.grid(axis='y')\n",
    "        plt.ylim(0.6, 1.0)\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.gca().xaxis.set_tick_params(rotation=45)\n",
    "\n",
    "    \n",
    "plot_metrics(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb01ca90-ed2d-4446-b43e-ffe8900f9aa0",
   "metadata": {},
   "source": [
    "**Question 4**\n",
    "\n",
    "Based on the performance metrics shown above, answer the following questions:\n",
    "\n",
    "1. Which are the best models?\n",
    "2. What can you say about the models using features as inputs?\n",
    "3. Bonus question: do you think it is *honest* to compare several models based on metrics computed on the test set in order to select the best one ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebe68b1-da41-4c69-ba33-aec5334188f4",
   "metadata": {},
   "source": [
    "**Question 5**\n",
    "\n",
    "Train and evaluate a model with a custom configuration. Can you obtain better performance than the previous models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6563c37-286d-472e-adef-242e089d3e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize the model configuration below.\n",
    "custom_config_name = 'custom'\n",
    "custom_config = {\n",
    "    'model': {\n",
    "        'name': 'cnn',\n",
    "        'config': {\n",
    "            'input_size': intervals.shape[1], \n",
    "            'output_size': targets.shape[1],\n",
    "            'n_layers': 2, \n",
    "            'n_initial_channels': 16,\n",
    "            'kernel_size': 5,\n",
    "        },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'name': 'adam',\n",
    "        'config': {'lr': 0.0001},\n",
    "    },\n",
    "}\n",
    "\n",
    "custom_model = train_model(custom_config_name, custom_config, intervals, targets, i_train, i_val, n_epochs, batch_size)\n",
    "custom_metrics = evaluate_model(custom_model, intervals, targets, i_train, i_val, i_test, batch_size)\n",
    "IPython.display.display(custom_metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
