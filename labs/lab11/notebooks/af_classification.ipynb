{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3d11189-b4fa-402d-92c1-e23d5e258ba6",
   "metadata": {},
   "source": [
    "# Atrial Fibrillation Classification\n",
    "\n",
    "The goal of this exercise is to train different conventional classification models to discriminate between atrial fibrillation and normal sinus rhythm from a sequence of interbeat intervals. We use interbeat intervals extracted from the Long Term AF Database (https://physionet.org/content/ltafdb/1.0.0/).\n",
    "\n",
    "We will train the following models on windows of interbeat intervals:\n",
    "\n",
    "* Decision tree\n",
    "* Support vector machine (SVM)\n",
    "* Naive Bayes\n",
    "\n",
    "The models will be trained on simple features derived from each window of interbeat intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43271ad3-6e0c-410b-b142-acce27564081",
   "metadata": {},
   "source": [
    "First, we import all the required packages, define global constants, and seed the random number generators to obtain reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b06315-c85f-4fe9-8a73-702e99267d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "\n",
    "import operator\n",
    "import pathlib\n",
    "import warnings\n",
    "import IPython.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.linear_model import Lasso\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "DATA_FILE = pathlib.Path('../data/ltafdb_intervals.npz')\n",
    "LOG_DIRECTORY = pathlib.Path('../logs/af_classification')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b7435e-81d1-4c73-b7b0-705e4ea080e1",
   "metadata": {},
   "source": [
    "Then, we load the windows of interbeat intervals and the corresponding labels. We also load the record identifiers. They will help to avoid using intervals from the same record for both training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18674b81-5bde-4f16-b642-98f960de1f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    with np.load(DATA_FILE) as data:\n",
    "        intervals = data['intervals']\n",
    "        labels = data['labels']\n",
    "        identifiers = data['identifiers']\n",
    "    return intervals, labels, identifiers\n",
    "\n",
    "\n",
    "intervals, labels, identifiers = load_data()\n",
    "targets = (labels == 'atrial_fibrillation').astype('float32')[:, None]\n",
    "window_size = intervals.shape[1]\n",
    "\n",
    "print(f'Number of windows: {intervals.shape[0]}')\n",
    "print(f'Window size: {window_size}')\n",
    "print(f'Window labels: {set(labels)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e0db28-03c0-4ef9-86db-27958f935fca",
   "metadata": {},
   "source": [
    "Here are a few examples of windows of interbeat intervals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eb179c-5887-4562-b26a-b2fb049e1eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_interval_examples(intervals, targets, n_examples=3):\n",
    "    normal_indices = np.random.choice(np.flatnonzero(targets == 0.0), n_examples, replace=False)\n",
    "    af_indices = np.random.choice(np.flatnonzero(targets == 1.0), n_examples, replace=False)\n",
    "    \n",
    "    def plot_intervals(ax, index):\n",
    "        ax.plot(np.cumsum(intervals[index]), intervals[index], '.-')\n",
    "        ax.grid(True)\n",
    "    \n",
    "    fig, axes = plt.subplots(n_examples, 2, sharex='all', sharey='all', squeeze=False, constrained_layout=True)\n",
    "    for i in range(n_examples):\n",
    "        plot_intervals(axes[i, 0], normal_indices[i])\n",
    "        plot_intervals(axes[i, 1], af_indices[i])\n",
    "    plt.setp(axes, ylim=(0.0, 3.0))\n",
    "    plt.setp(axes[-1, :], xlabel='Time [s]')\n",
    "    plt.setp(axes[:, 0], ylabel='IBI [s]')\n",
    "    axes[0, 0].set_title('Normal rhythm')\n",
    "    axes[0, 1].set_title('Atrial fibrillation')\n",
    "    \n",
    "    \n",
    "plot_interval_examples(intervals, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe3d08a-68e9-4a3f-98f4-31795e87cd89",
   "metadata": {},
   "source": [
    "The next step is to split the dataset into subsets for training, validation, and testing stratified by labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5126c623-7fb3-4b21-a6d9-e53c96fa3e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(identifiers, intervals, targets):\n",
    "    splitter = sklearn.model_selection.StratifiedGroupKFold(n_splits=5)\n",
    "    indices = list(map(operator.itemgetter(1), splitter.split(intervals, targets, identifiers)))\n",
    "    i_train = np.hstack(indices[:-2])\n",
    "    i_val = indices[-2]\n",
    "    i_test = indices[-1]\n",
    "    \n",
    "    assert not (set(identifiers[i_train]) & set(identifiers[i_val]))\n",
    "    assert not (set(identifiers[i_train]) & set(identifiers[i_test]))\n",
    "    assert not (set(identifiers[i_val]) & set(identifiers[i_test]))\n",
    "    assert set(identifiers[i_train]) | set(identifiers[i_val]) | set(identifiers[i_test]) == set(identifiers)\n",
    "    \n",
    "    return i_train, i_val, i_test\n",
    "\n",
    "\n",
    "i_train, i_val, i_test = split_data(identifiers, intervals, targets)\n",
    "\n",
    "\n",
    "def build_summary(subsets, targets):\n",
    "    data = []\n",
    "    for subset, y in zip(subsets, targets):\n",
    "        data.append({\n",
    "            'subset': subset,\n",
    "            'total_count': y.size,\n",
    "            'normal_count': np.sum(y == 0.0),\n",
    "            'af_count': np.sum(y == 1.0),\n",
    "            'normal_proportion': np.mean(y == 0.0),\n",
    "            'af_proportion': np.mean(y == 1.0),\n",
    "        })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "IPython.display.display(build_summary(('training', 'validation', 'testing'), (targets[i_train], targets[i_val], targets[i_test])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a1effb-438a-4d15-9d77-74693947a4e6",
   "metadata": {},
   "source": [
    "To better understand the dataset, we extract two features from each window of interbeat intervals: the mean and the standard deviation. We then plot these two features for the two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ee64b7-bbd0-4e09-9b38-dcd838a84fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_mean = np.mean(intervals, axis=1)\n",
    "f_std = np.std(intervals, axis=1)\n",
    "f_min = np.min(intervals, axis=1)\n",
    "f_max = np.max(intervals, axis=1)\n",
    "f_median = np.median(intervals, axis=1)\n",
    "features = pd.DataFrame({\n",
    "    'mean': f_mean,\n",
    "    'std': f_std,\n",
    "    'min': f_min,\n",
    "    'max': f_max, \n",
    "    'p2p': f_max - f_min, \n",
    "    'var': np.power(f_std, 2),\n",
    "    'median': f_median,\n",
    "    'mean_3': np.power(f_mean, 3),\n",
    "    'median_2': np.power(f_median, 2),\n",
    "    \n",
    "})\n",
    "\n",
    "\n",
    "def plot_features(f, y):\n",
    "    data = f.copy()\n",
    "    data['label'] = y.ravel()\n",
    "    data['label'] = data['label'].map({0.0: 'normal_rhythm', 1.0: 'atrial_fibrillation'})\n",
    "    sns.pairplot(data, hue='label', plot_kws={'s': 4})\n",
    "\n",
    "\n",
    "plot_features(features.iloc[i_train], targets[i_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32064a6-b9e8-4f39-90ec-bcb89212ee70",
   "metadata": {},
   "source": [
    "It is also possible to select the most relevant features using various methods. Here, we define implement three fature selection techniques: lasso, univariate, hybrid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99486c67-beab-49de-92a3-1a5b1bc8918a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureSelector:\n",
    "\n",
    "    def __init__(self, method, numbers):\n",
    "        self.method = method.lower()\n",
    "        self.numbers = numbers\n",
    "\n",
    "    def apply(self, features, targets):\n",
    "        features_names = [column for column in features.columns if column not in ['reference']]\n",
    "        features_selection = features.copy()\n",
    "        features_selection.insert(0, 'reference', targets)\n",
    "        features_selection = features_selection.dropna(axis=0, how='any', inplace=False)\n",
    "        ranks = self.rank_features(features_selection[features_names],\n",
    "                                   features_selection['reference'],\n",
    "                                   self.method)\n",
    "        del features_selection\n",
    "        return self.select_features(ranks, self.numbers)\n",
    "\n",
    "    @staticmethod\n",
    "    def select_features(ranks, feature_num):\n",
    "        ranks.sort_values(by='ranks', axis=0, ascending=False, inplace=True,\n",
    "                          kind='quicksort', ignore_index=True)\n",
    "        return ranks['feature_names'].iloc[: feature_num].tolist()\n",
    "\n",
    "    @staticmethod\n",
    "    def rank_features(features, reference, method):\n",
    "        def univariate_selection(data, ref):\n",
    "            selector = SelectKBest(f_regression, k=\"all\")\n",
    "            scores = selector.fit(data, ref).scores_\n",
    "            return scores / np.nansum(scores)\n",
    "\n",
    "        def lasso_selection(data, ref):\n",
    "            alphas = np.arange(0.01, 0.3, 0.01)\n",
    "            coefficients = np.empty([len(alphas), data.shape[1]])\n",
    "            for row, alpha in enumerate(alphas):\n",
    "                selector = SelectFromModel(Lasso(alpha=alpha), prefit=False)\n",
    "                coefficients[row, :] = selector.fit(data, ref).estimator_.coef_\n",
    "            coefficients = np.abs(coefficients)\n",
    "            real_ranks = np.nansum(coefficients, axis=0)\n",
    "            return real_ranks / np.nansum(real_ranks)\n",
    "\n",
    "        if method == 'lasso':\n",
    "            ranks = lasso_selection(features, reference)\n",
    "        elif method == 'univariate':\n",
    "            ranks = univariate_selection(features, reference)\n",
    "        elif method == 'hybrid':\n",
    "            rank_lasso = lasso_selection(features, reference)\n",
    "            rank_univariate = univariate_selection(features, reference)\n",
    "            rank_combined = rank_lasso + rank_univariate\n",
    "            ranks = rank_combined / np.nansum(rank_combined)\n",
    "        else:\n",
    "            raise TypeError(\"Feature selection method is not supported\")\n",
    "        return pd.DataFrame({\n",
    "            'feature_names': features.columns,\n",
    "            'ranks': ranks,\n",
    "        })\n",
    "\n",
    "feature_selection_method = 'lasso'\n",
    "feature_selection_numbers = 5\n",
    "features_list = FeatureSelector(feature_selection_method, feature_selection_numbers).apply(features.iloc[i_train], targets[i_train])\n",
    "print(f\"Selected features:{features_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d39dc6-e28f-4d1f-9e06-5f47d188bfbd",
   "metadata": {},
   "source": [
    "To classify atrial fibrillation and normal rhythm, we define the following models: Decision Tree, SVM, and Naive Bayes. To this end, we define a model builder class which provides a method to build the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041a16d1-cfc3-4fb4-9909-845cc36171e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelBuilder:\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config['model']\n",
    "\n",
    "    def apply(self):\n",
    "        return eval(f\"self._build_{self.config['name']}()\")\n",
    "\n",
    "    def _build_tree(self, max_depth=5):\n",
    "        if 'max_depth' in self.config.keys():\n",
    "            max_depth = self.config['max_depth']\n",
    "        return make_pipeline(\n",
    "            StandardScaler(),\n",
    "            DecisionTreeClassifier(max_depth=max_depth))\n",
    "\n",
    "    def _build_svm(self, kernel='rbf', gamma='scale', regularization=1):\n",
    "        if 'kernel' in self.config.keys():\n",
    "            kernel = self.config['kernel']\n",
    "        if 'gamma' in self.config.keys():\n",
    "            gamma = self.config['gamma']\n",
    "        if 'regularization' in self.config.keys():\n",
    "            regularization = self.config['regularization']\n",
    "        return make_pipeline(\n",
    "            StandardScaler(),\n",
    "            SVC(kernel=kernel, gamma=gamma, C=regularization,\n",
    "                probability=True))\n",
    "\n",
    "    def _build_bayes(self, var_smoothing=1e-09):\n",
    "        if 'var_smoothing' in self.config.keys():\n",
    "            var_smoothing = self.config['var_smoothing']\n",
    "        return make_pipeline(\n",
    "            StandardScaler(),\n",
    "            GaussianNB(var_smoothing=var_smoothing))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290ba56c-6e85-4a23-934c-c541ea867559",
   "metadata": {},
   "source": [
    "Now, we define a class for the training of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540cace1-b01e-463b-b023-0235dd381949",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config['feature']\n",
    "\n",
    "    def apply(self, model, features, reference, i_train):\n",
    "        features_list = self._get_features_list(list(features.columns))\n",
    "        features_train = features.copy()\n",
    "        features_train.insert(0, 'reference', reference)\n",
    "        features_train = features_train.iloc[i_train].copy()\n",
    "        features_train = features_train.dropna(axis=0, how='any', inplace=False,\n",
    "                                             subset=features_list + ['reference'])\n",
    "        return model.fit(\n",
    "            features_train[features_list].values, features_train['reference'].values)\n",
    "\n",
    "    def _get_features_list(self, current_features):\n",
    "        if 'all' in self.config['list']:\n",
    "            features_list = [feature for feature in current_features\n",
    "                             if feature not in self.config['exclusion'] + ['reference']]\n",
    "        else:\n",
    "            features_list = [feature for feature in\n",
    "                             self.config['list']\n",
    "                             if feature in current_features and feature not\n",
    "                             in self.config['exclusion'] + ['reference']]\n",
    "        return features_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099a311e-2c56-4e95-924b-57f5eed0f7ce",
   "metadata": {},
   "source": [
    "We also define a class to apply the trained models on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88a159a-dae8-49bf-808e-f4930be795c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTester:\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config['feature']\n",
    "\n",
    "    def apply(self, model, features):\n",
    "        features_list = self._get_features_list(list(features.columns))\n",
    "        inx = np.logical_not(\n",
    "            np.sum(np.isnan(features[features_list]), 1).astype(bool))\n",
    "        detections = np.zeros_like(inx)\n",
    "        detections[:] = np.nan\n",
    "        detections[inx] = model.predict(features[features_list].values[inx])\n",
    "        return pd.DataFrame({'prediction': detections})\n",
    "\n",
    "    def _get_features_list(self, current_features):\n",
    "        if 'all' in self.config['list']:\n",
    "            features_list = [feature for feature in current_features\n",
    "                             if feature not in self.config['exclusion'] + ['reference']]\n",
    "        else:\n",
    "            features_list = [feature for feature in self.config['list']\n",
    "                             if feature in current_features and feature not\n",
    "                             in self.config['exclusion'] + ['reference']]\n",
    "        return features_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bc8d2a-9ada-4468-aea3-36aedb560470",
   "metadata": {},
   "source": [
    "Inorder to evaluate the results of the models, we define an Evaluator class as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a9192a-3446-449c-b4a1-a9de29033cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def apply(self, result, reference, i_train, i_test):\n",
    "        result_bool = result.astype(bool)\n",
    "        reference_bool = reference.astype(bool)\n",
    "        metrics = []\n",
    "        for subset, indices in (('train', i_train), ('test', i_test)):\n",
    "            metrics.append({\n",
    "                'subset': subset,\n",
    "                **self.compute_performance_parameters(result_bool[indices], reference_bool[indices]),\n",
    "            })\n",
    "        return pd.DataFrame(metrics)\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_performance_parameters(result, reference):\n",
    "        def zero_division(a, b):\n",
    "            if b != 0:\n",
    "                return np.round(a / b, 2)\n",
    "            else:\n",
    "                return 0.00\n",
    "        result_not = np.logical_not(result)\n",
    "        reference_not = np.logical_not(reference)\n",
    "        tp = np.sum(result[reference])\n",
    "        fn = np.sum(result_not[reference])\n",
    "        tn = np.sum(result_not[reference_not])\n",
    "        fp = np.sum(result[reference_not])\n",
    "        return {\n",
    "            'tp': tp,\n",
    "            'fn': fn,\n",
    "            'tn': tn,\n",
    "            'fp': fp,\n",
    "            'sensitivity': zero_division(tp, tp + fn),\n",
    "            'specificity': zero_division(tn, tn + fp),\n",
    "            'accuracy': zero_division(tp + tn, tp + tn + fn + fp),\n",
    "            'precision': zero_division(tp, tp + fp)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1599366-82c7-4b18-b88c-3454d416e7b6",
   "metadata": {},
   "source": [
    "The final step before training and evaluating the models is to define the configurations of the different models.\n",
    "\n",
    "We will train the models with the following configurations:\n",
    "\n",
    "* Decision tree without features selection\n",
    "  * Using all the features\n",
    "  * max_depth: 3\n",
    "\n",
    "* Decision tree with features selection\n",
    "  * Using the selected features\n",
    "  * max_depth: 3\n",
    "\n",
    "* SVM without features selection\n",
    "  * Using all the features\n",
    "  * kernel: rbf\n",
    "  * gamma: scale\n",
    "  * regularization: 1\n",
    "\n",
    "* SVM with features selection\n",
    "  * Using the selected features\n",
    "  * kernel: rbf\n",
    "  * gamma: scale\n",
    "  * regularization: 1\n",
    "\n",
    "* Naive Bayes without features selection\n",
    "  * Using all the features\n",
    "  * var_smoothing: 1e-09\n",
    "  \n",
    "* Naive Bayes with features selection\n",
    "  * Using the selected features\n",
    "  * var_smoothing: 1e-09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbdff32-ddb1-46b6-a58c-8574b3ef304f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_features = []\n",
    "configs = {\n",
    "    'decision_tree_all_features': {\n",
    "        'feature': {\n",
    "            'list': 'all',\n",
    "            'exclusion': exclude_features,\n",
    "            'selection_method': [],\n",
    "            'selection_numbers': np.nan,\n",
    "        },\n",
    "        'model': {\n",
    "            'name': 'tree',\n",
    "            'max_depth': 15,\n",
    "        },\n",
    "    },\n",
    "    'decision_tree_selected_features': {\n",
    "        'feature': {\n",
    "            'list': features_list,\n",
    "            'exclusion': exclude_features,\n",
    "            'selection_method': feature_selection_method,\n",
    "            'selection_numbers': feature_selection_numbers,\n",
    "        },\n",
    "        'model': {\n",
    "            'name': 'tree',\n",
    "            'max_depth': 15,\n",
    "        },\n",
    "    },\n",
    "    'svm_all_features': {\n",
    "        'feature': {\n",
    "            'list': 'all',\n",
    "            'exclusion': exclude_features,\n",
    "            'selection_method': [],\n",
    "            'selection_numbers': np.nan,\n",
    "        },\n",
    "        'model': {\n",
    "            'name': 'svm',\n",
    "            'kernel': 'rbf',\n",
    "            'gamma': 'scale',\n",
    "            'regularization': 1, \n",
    "        },\n",
    "    },\n",
    "    'svm_selected_features': {\n",
    "        'feature': {\n",
    "            'list': features_list,\n",
    "            'exclusion': exclude_features,\n",
    "            'selection_method': feature_selection_method,\n",
    "            'selection_numbers': feature_selection_numbers,\n",
    "        },\n",
    "        'model': {\n",
    "            'name': 'svm',\n",
    "            'kernel': 'rbf',\n",
    "            'gamma': 'scale',\n",
    "            'regularization': 1, \n",
    "        },\n",
    "    },\n",
    "    'bayes_all_features': {\n",
    "        'feature': {\n",
    "            'list': 'all',\n",
    "            'exclusion': exclude_features,\n",
    "            'selection_method': [],\n",
    "            'selection_numbers': np.nan,\n",
    "        },\n",
    "        'model': {\n",
    "            'name': 'bayes',\n",
    "            'var_smoothing': 1e-09,\n",
    "        },\n",
    "    },\n",
    "    'bayes_selected_features': {\n",
    "        'feature': {\n",
    "            'list': features_list,\n",
    "            'exclusion': exclude_features,\n",
    "            'selection_method': feature_selection_method,\n",
    "            'selection_numbers': feature_selection_numbers,\n",
    "        },\n",
    "        'model': {\n",
    "            'name': 'bayes',\n",
    "            'var_smoothing': 1e-09,\n",
    "        },\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11787b46-7510-47e8-83d8-e39f030bf8d7",
   "metadata": {},
   "source": [
    "Now, we are ready to train the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd860b4-2b9e-4d78-9d4e-32ae24d2acb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "for name, config in configs.items():\n",
    "    print(f' * Training {name!r} model')\n",
    "    model = ModelBuilder(config).apply()\n",
    "    models[name] = ModelTrainer(config).apply(model, features, targets, i_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08eb6c14-86ec-4a14-b072-92db30ee43ac",
   "metadata": {},
   "source": [
    "Here, we evaluate the trained models on the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35fc462-24fa-46b0-b102-182a7bad4626",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {}\n",
    "for name, config in configs.items():\n",
    "    print(f' * Applying {name!r} model')\n",
    "    output[name] = ModelTester(config).apply(models[name], features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acce72a-9da5-4e28-b7d9-471ec9e66c93",
   "metadata": {},
   "source": [
    "Now that all models are trained we can evaluate them on the subsets for training, validation, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32b76c7-a838-4363-b703-c012b5f49055",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = []\n",
    "for name, config in configs.items():\n",
    "    print(f'Evaluating {name!r} model')\n",
    "    performance = Evaluator().apply(output[name]['prediction'].values, targets[:, 0], i_train, i_test)\n",
    "    performance.insert(0, 'model', name)\n",
    "    metrics.append(performance)\n",
    "print(\"\\n*** Performance report ***\\n\")\n",
    "metrics = pd.concat(metrics, axis=0, ignore_index=True)\n",
    "metrics = metrics.set_index(['model', 'subset'])\n",
    "index = metrics.index.get_level_values(0).unique()\n",
    "columns = pd.MultiIndex.from_product([metrics.columns, metrics.index.get_level_values(1).unique()])\n",
    "metrics = metrics.unstack().reindex(index=index, columns=columns)\n",
    "IPython.display.display(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5e5624-c3bc-474d-9fec-dbc51751d7ae",
   "metadata": {},
   "source": [
    "We can also plot the different metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14940ec-9126-4830-9a55-38a100665abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(data):\n",
    "    for metric in data.columns.get_level_values(0).unique():\n",
    "        if metric in ['count', 'tp', 'tn', 'fp', 'fn']:\n",
    "            continue\n",
    "        df = data[metric]\n",
    "        plt.figure(constrained_layout=True)\n",
    "        plt.gca().set_axisbelow(True)\n",
    "        df.plot(kind='bar', ylabel=metric, ax=plt.gca())\n",
    "        plt.grid(axis='y')\n",
    "        plt.ylim(0.6, 1.0)\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.gca().xaxis.set_tick_params(rotation=45)\n",
    "\n",
    "    \n",
    "plot_metrics(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39adab91-1d50-4a79-8664-7ce61724364d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
