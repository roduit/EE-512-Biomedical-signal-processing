{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f0077a4-3ea7-4eec-8677-9875bc920850",
   "metadata": {},
   "source": [
    "# Gait Classification\n",
    "\n",
    "The goal of this exercise is to classify three types of gaits from windows of stride intervals. The stride intervals we will use are extracted from the Gait in Aging and Disease Database (https://physionet.org/content/gaitdb/1.0.0/).\n",
    "\n",
    "This database includes stride intervals collected from 15 subjects: 5 healthy young adults, 5 healthy old adults, and 5 older adults with Parkinson's disease. Windows of 32 stride intervals (with 50% overlap) are already extracted from the raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2f9c1c-d86b-4517-9ee9-5364235a0b94",
   "metadata": {},
   "source": [
    "First, we import all required packages, define global constants, and seed the random number generators to obtain reproducible results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877de47c-e528-42bc-9126-b5e968d6794a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "\n",
    "import itertools\n",
    "import logging\n",
    "import operator\n",
    "import pathlib\n",
    "import warnings\n",
    "\n",
    "import IPython.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import seaborn as sns\n",
    "import sklearn.metrics\n",
    "import sklearn.model_selection\n",
    "import sklearn.preprocessing\n",
    "import torch\n",
    "\n",
    "\n",
    "DATA_FILE = pathlib.Path('../data/gaitdb_intervals.npz')\n",
    "LOG_DIRECTORY = pathlib.Path('../logs/gait_classification')\n",
    "\n",
    "\n",
    "# Disable logging for PyTorch Lightning to avoid too long outputs.\n",
    "logging.getLogger('pytorch_lightning').setLevel(logging.ERROR)\n",
    "\n",
    "# Seed random number generators for reproducible results.\n",
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e3841b-1c6b-4510-a657-2485c6f3d594",
   "metadata": {},
   "source": [
    "Then, we load the windows of stride intervals and the corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f860b30a-6cf4-4487-abb8-27f7d5fcb74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    with np.load(DATA_FILE) as data:\n",
    "        intervals = data['intervals']\n",
    "        labels = data['labels']\n",
    "        subjects = data['subjects']\n",
    "    return intervals, labels, subjects\n",
    "\n",
    "\n",
    "intervals, labels, subjects = load_data()\n",
    "\n",
    "print(f'Number of windows : {intervals.shape[0]}')\n",
    "print(f'Window size       : {intervals.shape[1]}')\n",
    "print(f'Classes           : {np.unique(labels)}')\n",
    "print(f'Subjects          : {np.unique(subjects)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb098955-f79d-438c-9bfd-127fc179d34e",
   "metadata": {},
   "source": [
    "We plot a few examples of stride interval windows for the different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550d9a27-cd18-4235-837b-f51b5ceab20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stride_examples(intervals, labels, n=3):\n",
    "    classes = np.unique(labels)\n",
    "    fig, axes = plt.subplots(n, len(classes), squeeze=False, \n",
    "                             sharex='all', sharey='all', constrained_layout=True)\n",
    "            \n",
    "    \n",
    "    def plot_intervals(ax, x):\n",
    "        t = np.cumsum(x)\n",
    "        ax.plot(t, x, '.-', linewidth=1)\n",
    "        ax.grid(True)\n",
    "        \n",
    "    for i, cls in enumerate(classes):\n",
    "        indices = np.flatnonzero(labels == cls)\n",
    "        indices = np.random.choice(indices, size=n)\n",
    "        for j in range(n):\n",
    "            if j == 0:\n",
    "                axes[j, i].set_title(cls.capitalize())\n",
    "            plot_intervals(axes[j, i], intervals[indices[j]])\n",
    "    \n",
    "    plt.setp(axes[:, 0], ylabel='Stride interval [s]')\n",
    "    plt.setp(axes[-1, :], xlabel='Time [s]')\n",
    "    \n",
    "    \n",
    "plot_stride_examples(intervals, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21fbc7e-5f54-451c-9af5-868716070821",
   "metadata": {},
   "source": [
    "**Question 1**\n",
    "\n",
    "Are there any visible differences between the three classes in these examples?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768cce8c-ed13-4379-946d-2fe69af906e2",
   "metadata": {},
   "source": [
    "Then, we split data into subsets for training (60%), validation (20%), and testing (20%) stratified by labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c78fae-c43c-404f-87eb-5c0c94128f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(labels):\n",
    "    n = labels.size\n",
    "    splitter = sklearn.model_selection.StratifiedKFold(n_splits=5, shuffle=True)\n",
    "    indices = list(map(operator.itemgetter(1), splitter.split(np.zeros((n, 1)), labels)))\n",
    "    i_train = np.hstack(indices[:-2])\n",
    "    i_val = indices[-2]\n",
    "    i_test = indices[-1]\n",
    "    \n",
    "    assert np.intersect1d(i_train, i_val).size == 0\n",
    "    assert np.intersect1d(i_train, i_test).size == 0\n",
    "    assert np.intersect1d(i_val, i_test).size == 0\n",
    "    assert np.all(np.sort(np.hstack((i_train, i_val, i_test))) == np.arange(n))\n",
    "    \n",
    "    return i_train, i_val, i_test\n",
    "\n",
    "\n",
    "i_train, i_val, i_test = split_data(labels)\n",
    "\n",
    "\n",
    "def build_summary(labels, indices):\n",
    "    classes = np.unique(labels)\n",
    "    data = []\n",
    "    for subset, i in indices:\n",
    "        y = labels[i]\n",
    "        data.append({'subset': subset, 'total_count': y.size})\n",
    "        for cls in classes:\n",
    "            data[-1][f'{cls}_count'] = np.sum(y == cls)\n",
    "    return pd.DataFrame(data)\n",
    "    \n",
    "\n",
    "IPython.display.display(build_summary(labels, (('train', i_train), ('val', i_val), ('test', i_test))))\n",
    "print(f'Subjects in training set   : {np.unique(subjects[i_train])}')\n",
    "print(f'Subjects in validation set : {np.unique(subjects[i_val])}')\n",
    "print(f'Subjects in test set       : {np.unique(subjects[i_test])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1fe3b2-248b-492e-b13b-751de4e73cc6",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Question 2**\n",
    "\n",
    "Comment on the method used to split data into training, validation, and test sets. Is is appropriate? What would be another approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6d347a-d506-4210-beb5-178246938ed9",
   "metadata": {},
   "source": [
    "To better understand the data, we visualize the mean and standard deviation of each window of stride intervals in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56025373-a766-4a46-aea0-7cd84d092c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stats(intervals, labels):\n",
    "    data = pd.DataFrame({\n",
    "        'label': labels,\n",
    "        'mean': np.mean(intervals, axis=1),\n",
    "        'std': np.std(intervals, axis=1),\n",
    "    })\n",
    "    sns.pairplot(data, hue='label')\n",
    "    \n",
    "    \n",
    "plot_stats(intervals[i_train], labels[i_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69855b6-a97e-45e7-8c7f-ff56150b5c3d",
   "metadata": {},
   "source": [
    "To ensure the training procedure is stable, we center and scale the windows of stride intervals such that they have approximately zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76205009-b291-483d-95ee-c84536f3f2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_intervals(intervals, i_train):\n",
    "    intervals = intervals - np.mean(intervals[i_train])\n",
    "    intervals = intervals / np.std(intervals[i_train])\n",
    "    return intervals\n",
    "\n",
    "\n",
    "def print_stats(intervals, i_train, i_val, i_test):\n",
    "    subsets = (\n",
    "        ('Training set', i_train),\n",
    "        ('Validation set', i_val), \n",
    "        ('Test set', i_test),\n",
    "    )\n",
    "    for name, i in subsets:\n",
    "        mu = np.mean(intervals[i])\n",
    "        sigma = np.std(intervals[i])\n",
    "        print(f'{name:14} : {mu:+.3f} Â± {sigma:.3f}')\n",
    "\n",
    "        \n",
    "print('Before scaling')\n",
    "print_stats(intervals, i_train, i_val, i_test)\n",
    "\n",
    "scaled_intervals = scale_intervals(intervals, i_train)\n",
    "\n",
    "print('\\nAfter scaling')\n",
    "print_stats(scaled_intervals, i_train, i_val, i_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b1e25f-3911-4a0e-8050-83df0955ae48",
   "metadata": {},
   "source": [
    "We also encode the labels with one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36b312e-e0ef-4ec0-b431-1357795a1931",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(labels):\n",
    "    categories = [np.unique(labels)]\n",
    "    encoder = sklearn.preprocessing.OneHotEncoder(categories=categories, sparse_output=False)\n",
    "    return encoder.fit_transform(labels[:, None])\n",
    "\n",
    "\n",
    "encoded_labels = encode_labels(labels)\n",
    "\n",
    "\n",
    "def print_encoded_labels(labels, encoded_labels, n=10):\n",
    "    df = pd.DataFrame(encoded_labels, columns=np.unique(labels))\n",
    "    df.insert(0, 'class', labels)\n",
    "    df = df.iloc[np.random.permutation(len(df)), :]\n",
    "    IPython.display.display(df.head(n))\n",
    "    \n",
    "    \n",
    "print_encoded_labels(labels, encoded_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3d791f-c96a-40d5-bac0-4617566577e8",
   "metadata": {},
   "source": [
    "We implement two models to classify the stride intervals: a multi-layer perceptron (MLP) and a convolutional neural network (CNN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684c6325-767a-4b35-9544-e6c8f9f34034",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlpModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 input_size, \n",
    "                 output_size, \n",
    "                 n_hidden_layers=1, \n",
    "                 n_units=128, \n",
    "                 dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.n_units = n_units\n",
    "        self.dropout = dropout\n",
    "        self.layers = self._build_layers()\n",
    "    \n",
    "    def _build_layers(self):\n",
    "        sizes = [self.input_size]\n",
    "        sizes.extend(itertools.repeat(self.n_units, self.n_hidden_layers))\n",
    "        layers = []\n",
    "        for i in range(self.n_hidden_layers):\n",
    "            layers.append(torch.nn.Linear(sizes[i], sizes[i + 1]))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            if 0.0 < self.dropout < 1.0:\n",
    "                layers.append(torch.nn.Dropout(self.dropout))\n",
    "        layers.append(torch.nn.Linear(sizes[-1], self.output_size))\n",
    "        return torch.nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "\n",
    "class CnnModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 input_size,\n",
    "                 output_size,\n",
    "                 n_layers=3,\n",
    "                 n_initial_channels=8,\n",
    "                 kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.n_initial_channels = n_initial_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.layers = self._build_layers()\n",
    "        \n",
    "    def _build_layers(self):\n",
    "        layers = []\n",
    "        \n",
    "        n_output_channels = 1\n",
    "        for i in range(self.n_layers):\n",
    "            n_input_channels = n_output_channels\n",
    "            n_output_channels = self.n_initial_channels * 2 ** i\n",
    "            layers.append(torch.nn.Conv1d(\n",
    "                in_channels=n_input_channels,\n",
    "                out_channels=n_output_channels,\n",
    "                kernel_size=self.kernel_size,\n",
    "                padding='same',\n",
    "            ))\n",
    "            layers.append(torch.nn.ReLU())\n",
    "            if i < self.n_layers - 1:\n",
    "                layers.append(torch.nn.MaxPool1d(kernel_size=2))\n",
    "            else:\n",
    "                layers.append(torch.nn.AdaptiveAvgPool1d(1))\n",
    "        layers.append(torch.nn.Flatten())\n",
    "        \n",
    "        layers.append(torch.nn.Linear(\n",
    "            in_features=self.n_initial_channels * 2 ** (self.n_layers - 1),\n",
    "            out_features=self.output_size,\n",
    "        ))\n",
    "        \n",
    "        return torch.nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], 1, x.shape[1])\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac70fa9-9b29-4c1e-9c13-0ca23e7d4268",
   "metadata": {},
   "source": [
    "We also define a classifier class and utility functions to make it easier to train and evaluate models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6091584e-74d4-4d62-a637-c39837dfdff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.config = config\n",
    "        self.model = self._build_model()\n",
    "        self.example_input_array = torch.zeros((1, self.model.input_size))\n",
    "    \n",
    "    def _build_model(self):\n",
    "        name = self.config['model']['name']\n",
    "        config = self.config['model'].get('config', {})\n",
    "        if name == 'mlp':\n",
    "            return MlpModel(**config)\n",
    "        elif name == 'cnn':\n",
    "            return CnnModel(**config)\n",
    "        else:\n",
    "            raise ValueError(f'unknown model: {name!r}')\n",
    "            \n",
    "    def configure_optimizers(self):\n",
    "        name = self.config['optimizer']['name']\n",
    "        config = self.config['optimizer'].get('config', {})\n",
    "        if name == 'sgd':\n",
    "            return torch.optim.SGD(self.parameters(), **config)\n",
    "        elif name == 'adam':\n",
    "            return torch.optim.Adam(self.parameters(), **config)\n",
    "        else:\n",
    "            raise ValueError(f'unknown optimizer: {name!r}')\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self._run_step(batch, 'train')\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._run_step(batch, 'val')\n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self._run_step(batch, 'test')\n",
    "        \n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        x, y = batch\n",
    "        return self.model(x)\n",
    "    \n",
    "    def _run_step(self, batch, subset):\n",
    "        x, y = batch\n",
    "        logits = self.model(x)\n",
    "        loss = torch.nn.functional.cross_entropy(logits, y)\n",
    "        acc = (torch.argmax(y, 1) == torch.argmax(logits, 1)).float().mean()\n",
    "        self.log_dict({\n",
    "            f'{subset}_loss': loss,\n",
    "            f'{subset}_acc': acc,\n",
    "        }, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "\n",
    "def build_loader(*tensors, batch_size=50, shuffle=False, n_workers=0):\n",
    "    dataset = torch.utils.data.TensorDataset(*map(torch.Tensor, tensors))\n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        num_workers=n_workers,\n",
    "    )\n",
    "\n",
    "\n",
    "def train_model(name, config, x, y, i_train, i_val, batch_size=50, n_epochs=10):\n",
    "    train_loader = build_loader(x[i_train], y[i_train], batch_size=batch_size, shuffle=True)\n",
    "    val_loader = build_loader(x[i_val], y[i_val], batch_size=batch_size)\n",
    "    classifier = Classifier(config)\n",
    "    print(pl.utilities.model_summary.ModelSummary(classifier, max_depth=-1))\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        trainer = pl.Trainer(\n",
    "            default_root_dir=LOG_DIRECTORY,\n",
    "            logger=pl.loggers.TensorBoardLogger(LOG_DIRECTORY, name),\n",
    "            enable_model_summary=False,\n",
    "            max_epochs=n_epochs,\n",
    "        )\n",
    "        trainer.fit(classifier, train_loader, val_loader)\n",
    "\n",
    "    return classifier\n",
    "\n",
    "\n",
    "def evaluate_model(model, x, y, i_train, i_val, i_test, batch_size=50):\n",
    "    loader = build_loader(x, y, batch_size=batch_size)\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        trainer = pl.Trainer(\n",
    "            default_root_dir=LOG_DIRECTORY,\n",
    "            logger=False,\n",
    "            enable_progress_bar=False,\n",
    "            enable_model_summary=False,\n",
    "        )\n",
    "        z = trainer.predict(model, loader)\n",
    "    z = np.vstack([u.numpy() for u in z])\n",
    "    \n",
    "    references = np.argmax(y, axis=1)\n",
    "    predictions = np.argmax(z, axis=1)\n",
    "    matrices = {}\n",
    "    for subset, indices in (('train', i_train), ('val', i_val), ('test', i_test)):\n",
    "        matrices[subset] = sklearn.metrics.confusion_matrix(\n",
    "            references[indices],\n",
    "            predictions[indices],\n",
    "        )\n",
    "        \n",
    "    return matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9037ed19-e0dd-4c38-9051-c2ea2371c374",
   "metadata": {},
   "source": [
    "Finally, we define functions to plot confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ebdeb1-b286-4b43-b879-ecda16077ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(c, labels=None, title=None):\n",
    "    c = np.asarray(c)\n",
    "\n",
    "    fig = plt.figure(figsize=(5, 4), constrained_layout=True)\n",
    "    image = plt.imshow(c, cmap='Blues', interpolation='nearest')\n",
    "\n",
    "    threshold = (c.min() + c.max()) / 2\n",
    "    for i, j in itertools.product(range(c.shape[0]), repeat=2):\n",
    "        if c[i, j] < threshold:\n",
    "            color = image.cmap(image.cmap.N)\n",
    "        else:\n",
    "            color = image.cmap(0)\n",
    "        text = format(c[i, j], '.2g')\n",
    "        if c.dtype.kind != 'f':\n",
    "            integer_text = format(c[i, j], 'd')\n",
    "            if len(integer_text) < len(text):\n",
    "                text = integer_text\n",
    "        plt.text(j, i, text, color=color, ha='center', va='center')\n",
    "\n",
    "    if labels is not None:\n",
    "        plt.xticks(np.arange(c.shape[-1]), labels, rotation=45, ha='right')\n",
    "        plt.yticks(np.arange(c.shape[-1]), labels)\n",
    "    plt.xlabel('Predictions')\n",
    "    plt.ylabel('References')\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "\n",
    "\n",
    "\n",
    "def plot_confusion_matrices(matrices, labels):\n",
    "    for subset in ('train', 'val', 'test'):\n",
    "        c = matrices[subset]\n",
    "        accuracy = np.trace(c) / c.sum()\n",
    "        c = c / np.sum(c, axis=1, keepdims=True)\n",
    "        title = f'{subset.capitalize()} set (accuracy = {accuracy:.3f})'\n",
    "        plot_confusion_matrix(c, labels=labels, title=title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc746efd-b9cc-477c-ad3b-64855d763264",
   "metadata": {},
   "source": [
    "We use TensorBoard to visualize performance metrics during training.\n",
    "\n",
    "If you prefer to view TensorBoard in a separate window, you can open http://localhost:6006/ in your web browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2165ad86-f3eb-492a-b4fb-42cd2a4869b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir ../logs/gait_classification --port 6006"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7891366d-e3e9-4531-bae8-b805dd773125",
   "metadata": {},
   "source": [
    "We define a configuration for an MLP model with the following layers:\n",
    "\n",
    "* Dense layer (output size = 128)\n",
    "* ReLU activation\n",
    "* Dense layer (output size = 128)\n",
    "* ReLU activation\n",
    "* Dense layer (output size = 3)\n",
    "\n",
    "We then train the MLP model with stochastic gradient descent (SGD) and evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e66ceae-bd20-4fe8-b55e-a25d7bb1fa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "n_epochs = 200\n",
    "\n",
    "mlp_config = {\n",
    "    'model': {\n",
    "        'name': 'mlp',\n",
    "        'config': {\n",
    "            'input_size': scaled_intervals.shape[1],\n",
    "            'output_size': encoded_labels.shape[1],\n",
    "            'n_hidden_layers': 2,\n",
    "            'n_units': 128,\n",
    "        },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'name': 'sgd',\n",
    "        'config': {\n",
    "            'lr': 0.0001,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "mlp_model = train_model(\n",
    "    name='mlp', \n",
    "    config=mlp_config, \n",
    "    x=scaled_intervals, \n",
    "    y=encoded_labels, \n",
    "    i_train=i_train, \n",
    "    i_val=i_val,\n",
    "    batch_size=batch_size,\n",
    "    n_epochs=n_epochs,\n",
    ")\n",
    "\n",
    "mlp_matrices = evaluate_model(\n",
    "    model=mlp_model, \n",
    "    x=scaled_intervals, \n",
    "    y=encoded_labels, \n",
    "    i_train=i_train, \n",
    "    i_val=i_val,\n",
    "    i_test=i_test,\n",
    ")\n",
    "\n",
    "plot_confusion_matrices(mlp_matrices, np.unique(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad4c150-6d92-4fc7-90c1-fa56e2b74d25",
   "metadata": {},
   "source": [
    "**Question 3**\n",
    "\n",
    "Based on the metrics shown in TensorBoard, comment on the training procedure. Does the model overfit?\n",
    "\n",
    "Based on the confusion matrices, what is the main issue of the model? What is a probable explanation for these results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ae3def-aaa6-4875-b2f5-588a31a0cfb8",
   "metadata": {},
   "source": [
    "Next, we train and evaluate the same MLP model architecture except that this time we use SGD with momentum for optimizing the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3967fb7b-1a25-4cbf-94cb-747faf55bef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_momentum_config = {\n",
    "    'model': {\n",
    "        'name': 'mlp',\n",
    "        'config': {\n",
    "            'input_size': scaled_intervals.shape[1],\n",
    "            'output_size': encoded_labels.shape[1],\n",
    "            'n_hidden_layers': 2,\n",
    "            'n_units': 128,\n",
    "        },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'name': 'sgd',\n",
    "        'config': {\n",
    "            'lr': 0.0001,\n",
    "            'momentum': 0.9,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "mlp_momentum_model = train_model(\n",
    "    name='mlp_momentum', \n",
    "    config=mlp_momentum_config, \n",
    "    x=scaled_intervals, \n",
    "    y=encoded_labels, \n",
    "    i_train=i_train, \n",
    "    i_val=i_val,\n",
    "    batch_size=batch_size,\n",
    "    n_epochs=n_epochs,\n",
    ")\n",
    "\n",
    "mlp_momentum_matrices = evaluate_model(\n",
    "    model=mlp_momentum_model, \n",
    "    x=scaled_intervals, \n",
    "    y=encoded_labels, \n",
    "    i_train=i_train, \n",
    "    i_val=i_val,\n",
    "    i_test=i_test,\n",
    ")\n",
    "\n",
    "plot_confusion_matrices(mlp_momentum_matrices, np.unique(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a50506-aacc-49e5-a908-9486a02855d7",
   "metadata": {},
   "source": [
    "**Question 4**\n",
    "\n",
    "What is the main difference of using momentum for training? Does the model overfit? Does momentum help to significantly improve the model accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43794c1f-4e3a-41aa-afef-3ec883eeead2",
   "metadata": {},
   "source": [
    "Now, we use the same MLP architecture again, but we use the [Adam optimizer](https://arxiv.org/abs/1412.6980) for training instead of SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a537d5f-d8fa-44a9-aa02-41615d4c9cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_adam_config = {\n",
    "    'model': {\n",
    "        'name': 'mlp',\n",
    "        'config': {\n",
    "            'input_size': scaled_intervals.shape[1],\n",
    "            'output_size': encoded_labels.shape[1],\n",
    "            'n_hidden_layers': 2,\n",
    "            'n_units': 128,\n",
    "        },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'name': 'adam',\n",
    "        'config': {\n",
    "            'lr': 0.0001,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "mlp_adam_model = train_model(\n",
    "    name='mlp_adam', \n",
    "    config=mlp_adam_config, \n",
    "    x=scaled_intervals, \n",
    "    y=encoded_labels, \n",
    "    i_train=i_train, \n",
    "    i_val=i_val,\n",
    "    batch_size=batch_size,\n",
    "    n_epochs=n_epochs,\n",
    ")\n",
    "\n",
    "mlp_adam_matrices = evaluate_model(\n",
    "    model=mlp_adam_model, \n",
    "    x=scaled_intervals, \n",
    "    y=encoded_labels, \n",
    "    i_train=i_train, \n",
    "    i_val=i_val,\n",
    "    i_test=i_test,\n",
    ")\n",
    "\n",
    "plot_confusion_matrices(mlp_adam_matrices, np.unique(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cab301-6e5e-4c72-9858-19840f239a57",
   "metadata": {},
   "source": [
    "**Question 5**\n",
    "\n",
    "What are the main differences of the MLP model trained with the Adam optimized compared to the models trained with SGD (with and without momentum)? Does the model overfit? Does the Adam optimizer help to achieve better overall performance (as shown in the confusion matrices)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d537b6a0-d82f-44a1-b94a-db2ae5911bc4",
   "metadata": {},
   "source": [
    "Finally, we train a CNN model with the Adam optimizer. This model includes the following layers:\n",
    "\n",
    "* Convolutional layer (kernel size = 3, output size = 32, output channels = 8)\n",
    "* ReLU activation\n",
    "* Max pooling (output size = 16, output channels = 8)\n",
    "* Convolutional layer (kernel size = 3, output size = 16, output channels = 16)\n",
    "* ReLU activation\n",
    "* Max pooling (output size = 8, output channels = 16)\n",
    "* Convolutional layer (kernel size = 3, output size = 8, output channels = 32)\n",
    "* ReLU activation\n",
    "* Global average pooling (output size = 32)\n",
    "* Dense layer (output size = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff878d9e-f49e-4829-b00b-a29744378f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_config = {\n",
    "    'model': {\n",
    "        'name': 'cnn',\n",
    "        'config': {\n",
    "            'input_size': scaled_intervals.shape[1],\n",
    "            'output_size': encoded_labels.shape[1],\n",
    "            'n_layers': 3,\n",
    "            'n_initial_channels': 8,\n",
    "            'kernel_size': 3,\n",
    "        },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'name': 'adam',\n",
    "        'config': {\n",
    "            'lr': 0.0001,\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "cnn_model = train_model(\n",
    "    name='cnn', \n",
    "    config=cnn_config, \n",
    "    x=scaled_intervals, \n",
    "    y=encoded_labels, \n",
    "    i_train=i_train, \n",
    "    i_val=i_val,\n",
    "    batch_size=batch_size,\n",
    "    n_epochs=n_epochs,\n",
    ")\n",
    "\n",
    "cnn_matrices = evaluate_model(\n",
    "    model=cnn_model, \n",
    "    x=scaled_intervals, \n",
    "    y=encoded_labels, \n",
    "    i_train=i_train, \n",
    "    i_val=i_val,\n",
    "    i_test=i_test,\n",
    ")\n",
    "\n",
    "plot_confusion_matrices(cnn_matrices, np.unique(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be620c38-6ee3-4659-b1be-f38da2929efc",
   "metadata": {},
   "source": [
    "**Question 6**\n",
    "\n",
    "What are the main differences of the CNN model with respect the MLP models trained with different optimizers? Does the CNN model overfit?\n",
    "\n",
    "Can you think of a few reasons to explain why the MLP model trained with the Adam optimizer and the CNN model behave differently in terms of overfitting?\n",
    "\n",
    "Overall, what is the main limitation of this dataset of stride intervals for training neural network models?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
